{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMAjMKzMoF15U3AfRimgscG",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Classymotion/Smilegate/blob/main/gpt2_fine_tuning_simple.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HD80hoYI8dAx",
        "outputId": "2a0a8a0c-f1b2-47aa-b815-dc52a458ea53"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.28.1-py3-none-any.whl (7.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.0/7.0 MB\u001b[0m \u001b[31m53.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.65.0)\n",
            "Collecting huggingface-hub<1.0,>=0.11.0\n",
            "  Downloading huggingface_hub-0.14.1-py3-none-any.whl (224 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m224.5/224.5 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.27.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
            "  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m69.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.22.4)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2022.10.31)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (4.5.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (2023.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.12)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (1.26.15)\n",
            "Installing collected packages: tokenizers, huggingface-hub, transformers\n",
            "Successfully installed huggingface-hub-0.14.1 tokenizers-0.13.3 transformers-4.28.1\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers\n",
        "import torch\n",
        "from transformers import GPT2Tokenizer, GPT2LMHeadModel, AdamW"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "qa_pairs = [\n",
        "['Q: How to delete the account A: IOS: Go to settings on the app, click on Edit, Click on Logout, Delete the Account Android: Go to settings, Click on the three dots, Click on Logout, Delete the Account.'],\n",
        "['Q: How to deactivate my account A: IOS: Go to settings on the app, click on Edit, Click on Logout, Delete the Account Android: Go to settings, Click on the three dots, Click on Logout, Delete the Account.'],\n",
        "['Q: How i can remove my nmbr A: IOS: Go to settings on the app, click on Edit, Click on Logout, Delete the Account Android: Go to settings, Click on the three dots, Click on Logout, Delete the Account.'],\n",
        "['Q: How to delete an account A: IOS: Go to settings on the app, click on Edit, Click on Logout, Delete the Account Android: Go to settings, Click on the three dots, Click on Logout, Delete the Account.']\n",
        "]\n",
        "\n"
      ],
      "metadata": {
        "id": "MjiCQKOH87pG"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
        "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
        "'''\n",
        "\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM \n",
        "tokenizer = AutoTokenizer.from_pretrained(\n",
        "  'kakaobrain/kogpt', revision='KoGPT6B-ryan1.5b-float16',  # or float32 version: revision=KoGPT6B-ryan1.5b\n",
        "  bos_token='[BOS]', eos_token='[EOS]', unk_token='[UNK]', pad_token='[PAD]', mask_token='[MASK]'\n",
        ")\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "  'kakaobrain/kogpt', revision='KoGPT6B-ryan1.5b-float16',  # or float32 version: revision=KoGPT6B-ryan1.5b\n",
        "  pad_token_id=tokenizer.eos_token_id,\n",
        "  torch_dtype='auto'#, low_cpu_mem_usage=True\n",
        ").to(device='cuda', non_blocking=True)\n",
        "text = ''\n",
        "for question in qa_pairs:\n",
        "  text += f'{question} {tokenizer.eos_token}'"
      ],
      "metadata": {
        "id": "OpwrDFqU8zd1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_ids = tokenizer.encode(text, add_special_tokens=True, return_tensors='pt')"
      ],
      "metadata": {
        "id": "Ot4MiD6E9rc8"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
        "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.9)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4JKXbYO49xO1",
        "outputId": "b128d8a4-6cc1-4d30-fa46-c93174916bfa"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model.to(device)\n",
        "model.train()\n",
        "\n",
        "for epoch in range(500):\n",
        "  optimizer.zero_grad()\n",
        "  loss = model(input_ids.to(device), labels=input_ids.to(device))[0]\n",
        "  loss.backward()\n",
        "  optimizer.step()\n",
        "  scheduler.step()\n",
        "\n",
        "  if epoch % 100 == 0:\n",
        "    print(f'Epoch {epoch}, Loss {loss.item()}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m397aLbR94FV",
        "outputId": "777bd73f-8e7d-4d14-ce11-1a206fb57c1a"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0, Loss 0.7477065324783325\n",
            "Epoch 100, Loss 0.630384624004364\n",
            "Epoch 200, Loss 0.6000802516937256\n",
            "Epoch 300, Loss 0.6077677011489868\n",
            "Epoch 400, Loss 0.553576648235321\n",
            "Epoch 500, Loss 0.60533607006073\n",
            "Epoch 600, Loss 0.5997542142868042\n",
            "Epoch 700, Loss 0.5424821972846985\n",
            "Epoch 800, Loss 0.5770900249481201\n",
            "Epoch 900, Loss 0.5824160575866699\n",
            "Epoch 1000, Loss 0.5878759622573853\n",
            "Epoch 1100, Loss 0.5695072412490845\n",
            "Epoch 1200, Loss 0.5838069319725037\n",
            "Epoch 1300, Loss 0.5799137949943542\n",
            "Epoch 1400, Loss 0.5606545805931091\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Olf7E433-uB8",
        "outputId": "081f1da4-37f0-4d45-ac1f-b08198389e71"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GPT2LMHeadModel(\n",
              "  (transformer): GPT2Model(\n",
              "    (wte): Embedding(50257, 768)\n",
              "    (wpe): Embedding(1024, 768)\n",
              "    (drop): Dropout(p=0.1, inplace=False)\n",
              "    (h): ModuleList(\n",
              "      (0-11): 12 x GPT2Block(\n",
              "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): GPT2Attention(\n",
              "          (c_attn): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): GPT2MLP(\n",
              "          (c_fc): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (act): NewGELUActivation()\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "  )\n",
              "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_response(question):\n",
        "  # Encode the question using the tokenizer\n",
        "  input_ids = tokenizer.encode(question + \" <|question|>\", add_special_tokens=False, return_tensors='pt').to(device)\n",
        "\n",
        "  # Generate the answer using the model\n",
        "  sample_output = model.generate(input_ids, do_sample=True, max_length=100, top_k=20, top_p=1.0)\n",
        "\n",
        "  # Decode the generated answer using the tokenizer\n",
        "  answer = tokenizer.decode(sample_output[0], skip_special_tokens=True)\n",
        "\n",
        "  # Split the generated answer into individual sentences\n",
        "  sentences = answer.split('. ')\n",
        "\n",
        "  # Look for the sentence that contains the answer to the question\n",
        "  for sentence in sentences:\n",
        "      if question in sentence:\n",
        "          return sentence\n",
        "\n",
        "  # If no sentence contains the answer, return the full generated answer\n",
        "  return answer"
      ],
      "metadata": {
        "id": "KXOVt5FC--B4"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "question = 'How can I delete my account?'\n",
        "response = generate_response(question)\n",
        "print(f'{question}\\n {response}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "peanJ_4X_Ptc",
        "outputId": "13ffa994-ed00-4a35-8786-ff135dc2fe2b"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "How can I delete my account?\n",
            " How can I delete my account? <|question|>If you delete your account, you'll lose your account (and the site), and it goes on to be suspended indefinitely.</| [1]\n"
          ]
        }
      ]
    }
  ]
}